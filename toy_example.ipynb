{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "import torch\n",
    "from torch.optim.sgd import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_status(grad, start, end, prefix):\n",
    "    if isinstance(grad, torch.Tensor):\n",
    "        grad = grad.item()\n",
    "    if isinstance(start, torch.Tensor):\n",
    "        start = start.item()\n",
    "    if isinstance(end, torch.Tensor):\n",
    "        end = end.item()\n",
    "\n",
    "    return print(f'{prefix} Gradient: {grad}. (Parameter: {start} -> {end}) {\"not updated\" if start == end else \"\"}')\n",
    "\n",
    "input_start = 2.0\n",
    "model_start = 3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient: 2.0. (Parameter: 3.0 -> 1.0) \n"
     ]
    }
   ],
   "source": [
    "# 1) usual procedure\n",
    "input = torch.tensor(2.0, requires_grad=False)\n",
    "model = torch.tensor(3.0, requires_grad=True)\n",
    "loss = input * model\n",
    "sgd = SGD(params=[model], lr=1.0)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "print_status(model.grad, model_start, model, 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient: 2.0. (Parameter: 3.0 -> 1.0) \n",
      "Input Gradient: 3.0. (Parameter: 2.0 -> -1.0) \n"
     ]
    }
   ],
   "source": [
    "# 2) with input and model together optimized\n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "model = torch.tensor(3.0, requires_grad=True)\n",
    "loss = input * model\n",
    "sgd = SGD(params=[input, model], lr=1.0)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "print_status(model.grad, model_start, model, 'Model')\n",
    "print_status(input.grad, input_start, input, 'Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient: 2.0. (Parameter: 3.0 -> 1.0) \n",
      "Input Gradient: 3.0. (Parameter: 2.0 -> 2.0) not updated\n"
     ]
    }
   ],
   "source": [
    "# 3) update only model but populate the gradient for input\n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "model = torch.tensor(3.0, requires_grad=True)\n",
    "loss = input * model\n",
    "sgd = SGD(params=[model], lr=1.0)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "print_status(model.grad, model_start, model, 'Model')\n",
    "print_status(input.grad, input_start, input, 'Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update1\n",
      "Model Gradient: 2.0. (Parameter: 3.0 -> 1.0) \n",
      "Input Gradient: 3.0. (Parameter: 2.0 -> 2.0) not updated\n"
     ]
    }
   ],
   "source": [
    "# 4) update only model but populate the gradient for input\n",
    "# 4.1) inner loop update 1\n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "model = torch.tensor(3.0, requires_grad=True)\n",
    "input_start = input.item()\n",
    "model_start = model.item()\n",
    "loss = input * model\n",
    "sgd = SGD(params=[model], lr=1.0)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "print('Update1')\n",
    "print_status(model.grad, model_start, model, 'Model')\n",
    "print_status(input.grad, input_start, input, 'Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update2\n",
      "Model Gradient: 4.0. (Parameter: 1.0 -> -3.0) \n",
      "Input Gradient: 0.0. (Parameter: 2.0 -> 2.0) not updated\n"
     ]
    }
   ],
   "source": [
    "input.grad = torch.zeros_like(input.grad) # empty grad\n",
    "model.grad = torch.zeros_like(model.grad) # empty grad\n",
    "input_start = input.item()\n",
    "model_start = model.item()\n",
    "#  4.2) inner loop update 2\n",
    "new_input = torch.tensor(4.0, requires_grad=False)\n",
    "new_loss = new_input * model\n",
    "sgd = SGD(params=[model], lr=1.0)\n",
    "new_loss.backward()\n",
    "sgd.step()\n",
    "print('Update2')\n",
    "print_status(model.grad, model_start, model, 'Model')\n",
    "print_status(input.grad, input_start, input, 'Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem is input grad equal to zero. No gradient flows back to input.\n",
    "It doesn't work because SGD update is in-place op without any computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "import torch\n",
    "from torch.optim.sgd import SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiplyModule(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MultiplyModule, self).__init__()\n",
    "        self.param = nn.Parameter(model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.param * x\n",
    "\n",
    "    \n",
    "def manual_update(model, lr, grads=None):\n",
    "    if grads is not None:\n",
    "        params = list(model.parameters())\n",
    "        if not len(grads) == len(list(params)):\n",
    "            msg = 'WARNING:manual_update(): Parameters and gradients have different length. ('\n",
    "            msg += str(len(params)) + ' vs ' + str(len(grads)) + ')'\n",
    "            print(msg)\n",
    "        for p, g in zip(params, grads):\n",
    "            if g is not None:\n",
    "                p.update = - lr * g\n",
    "    return update_module(model)\n",
    "\n",
    "def update_module(module, updates=None, memo=None):\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "    if updates is not None:\n",
    "        params = list(module.parameters())\n",
    "        if not len(updates) == len(list(params)):\n",
    "            msg = 'WARNING:update_module(): Parameters and updates have different length. ('\n",
    "            msg += str(len(params)) + ' vs ' + str(len(updates)) + ')'\n",
    "            print(msg)\n",
    "        for p, g in zip(params, updates):\n",
    "            p.update = g\n",
    "\n",
    "    # Update the params\n",
    "    for param_key in module._parameters:\n",
    "        p = module._parameters[param_key]\n",
    "        if p in memo:\n",
    "            module._parameters[param_key] = memo[p]\n",
    "        else:\n",
    "            if p is not None and hasattr(p, 'update') and p.update is not None:\n",
    "                updated = p + p.update\n",
    "                p.update = None\n",
    "                memo[p] = updated\n",
    "                module._parameters[param_key] = updated\n",
    "\n",
    "    # Second, handle the buffers if necessary\n",
    "    for buffer_key in module._buffers:\n",
    "        buff = module._buffers[buffer_key]\n",
    "        if buff in memo:\n",
    "            module._buffers[buffer_key] = memo[buff]\n",
    "        else:\n",
    "            if buff is not None and hasattr(buff, 'update') and buff.update is not None:\n",
    "                updated = buff + buff.update\n",
    "                buff.update = None\n",
    "                memo[buff] = updated\n",
    "                module._buffers[buffer_key] = updated\n",
    "\n",
    "    # Then, recurse for each submodule\n",
    "    for module_key in module._modules:\n",
    "        module._modules[module_key] = update_module(\n",
    "            module._modules[module_key],\n",
    "            updates=None,\n",
    "            memo=memo,\n",
    "        )\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient: 2.0. (Parameter: 3.0 -> 1.0) \n",
      "Input Gradient: 3.0. (Parameter: 2.0 -> 2.0) not updated\n"
     ]
    }
   ],
   "source": [
    "# manual gradient\n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "model_param = torch.tensor(3.0, requires_grad=True)\n",
    "model = MultiplyModule(model_param)\n",
    "sgd = SGD(params=[input], lr=1.0)\n",
    "\n",
    "input_start = input.item()\n",
    "model_start = model.param.item()\n",
    "loss = model(input)\n",
    "# manual gradient call\n",
    "gradients = grad(loss,   # loss\n",
    "                 list(model.parameters())+[input], # parameters\n",
    "                 retain_graph=True,\n",
    "                 create_graph=True,\n",
    "                 allow_unused=False)\n",
    "model_grad, input_grad = gradients[:1], gradients[1:]\n",
    "model = manual_update(model=model, lr=1.0, grads=model_grad)\n",
    "# p.grad = - lr * grad\n",
    "print_status(model_grad[0], model_start, model.param, 'Model')\n",
    "print_status(input_grad[0], input_start, input, 'Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input_start = input.item()\n",
    "model_start = model.param.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient: 2.0. (Parameter: 1.0 -> 1.0) not updated\n",
      "Input Gradient: -4.0. (Parameter: 2.0 -> 6.0) \n"
     ]
    }
   ],
   "source": [
    "new_input = torch.tensor(4.0, requires_grad=False)\n",
    "loss = model(new_input)\n",
    "loss.backward()\n",
    "sgd.step()\n",
    "print_status(model_grad[0], model_start, model.param, 'Model')\n",
    "print_status(input.grad, input_start, input, 'Input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
